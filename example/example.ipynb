{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# faculty-distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage\n",
    "In this example, we will train an XGBoost model on the Iris dataset. We will perform a grid search of the hyperparameters `learning_Rate` and `max_depth` distributed over mutilple servers using the Jobs functionality of the Faculty Platform. The accuracy metric of each classifier will be logged in the Experiments tab of Faculty Platform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import faculty_distributed\n",
    "from faculty import client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Environments "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create evironments that install the faculty-distributed package and the xgboost package. It is recommended that the faculty-distributed package is installed through it's own environment, as it can then be kept clean for use on any distributed job. To create these environments, go to the Environments tab of the Faculty Platform and create two new environments called `faculty_distributed` and `xgboost` that install `faculty_distributed` and `xgboost` respectively using `pip`. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Job "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a new job definition named `distributed_example`. In the `COMMAND` section, paste the following:\n",
    "\n",
    "```bash\n",
    "faculty_distributed_job $path $worker_id\n",
    "```\n",
    "\n",
    "Then, add a `PARAMETER` with the name `path`, of type `text` and ensure that the `Make field mandatory` box is checked. Create another `PARAMETER` named `worker_id` of type `text` and ensure that the `Make field mandatory` box is checked.\n",
    "\n",
    "Finally, under `SERVER SETTINGS`, add `faculty_distributed` to the `ENVIRONMENTS` section. Note that any libraries used in the function to be executed that are not installed automatically on Faculty servers need to be installed on the job server via a separate environment, so here we also need to add the `xgboost` environment we created earlier.\n",
    "\n",
    "Depending on the level of parallelisation required and how long each function takes to run it may be better to use dedicated rather than shared instances. To achieve this, click on `Large and GPU servers` under `SERVER RESOURCES`, and select an appropriate server type from the dropdown menu.\n",
    "\n",
    "Remember to click `SAVE` when you are finished."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get project and job IDs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The project ID is stored as an environment variable. The job ID can be found using the `job_name_to_job_id` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = os.getenv(\"FACULTY_PROJECT_ID\")\n",
    "job_id = faculty_distributed.job_name_to_job_id(\"distributed_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create temporary directory to save the Iris data as binary files during this example notebook. This directory will be deleted at the end of the notebook. (Note: it is not necessary to save files in binary format, but it is convenient in this example as xgboost uses its own `DMatrix` data structure.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = tempfile.mkdtemp(prefix=\"/project/temp-data-\")\n",
    "\n",
    "data = load_iris()\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.data, data.target)\n",
    "\n",
    "xg_train = xgb.DMatrix(x_train, label=y_train)\n",
    "xg_test = xgb.DMatrix(x_test, label=y_test)\n",
    "\n",
    "xg_train.save_binary(os.path.join(path, \"train.buffer\"))\n",
    "xg_test.save_binary(os.path.join(path, \"test.buffer\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to be executed on distributed workers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function to train an XGBoost classifier. The inputs to this function are the path to where the data is saved and the two parameters over which we will be searching over. These parameters are the learning rate and the max_depth of the tree. The function logs the accuracy of the model and outputs the predictions made by this classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(datapath, learning_rate, max_depth):\n",
    "    \"\"\"Train an XGBoost Classifer and return predicted classes.\n",
    "    \n",
    "    Parameter values and accuracy as logged in mlflow.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    datapath: str\n",
    "        path from which data will be read\n",
    "    learning_rate: float\n",
    "        learning rate for XGB Classifier\n",
    "    max_depth: int\n",
    "        maximum depth of XGB tree\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    predictions: list\n",
    "        list of predicted classes evaluated on test set\n",
    "    \"\"\"\n",
    "    mlflow.set_experiment(\"Iris XGB classifier\")\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"learning_rate\", str(learning_rate))\n",
    "        mlflow.log_param(\"max_depth\", str(max_depth))\n",
    "        \n",
    "        xg_train = xgb.DMatrix(os.path.join(datapath, \"train.buffer\"))\n",
    "        xg_test = xgb.DMatrix(os.path.join(datapath, \"test.buffer\"))\n",
    "\n",
    "        param = {}\n",
    "        param['objective'] =  'multi:softmax'\n",
    "        param['num_class'] = len(np.unique(y_train))\n",
    "        param['learning_rate'] = learning_rate\n",
    "        param['max_depth'] = max_depth\n",
    "\n",
    "        bst = xgb.train(param, xg_train)\n",
    "        predictions = bst.predict(xg_test)\n",
    "        \n",
    "        mlflow.log_metric(\"accuracy\", accuracy_score(y_test, predictions))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define arguments list to be sent to each worker "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give list of arguments to for function to run. Here we are performing a grid search of hyperparameters. The path to the data is also passed. (Note: parameter values are chosen here to deliberately produce different accuracy scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.1, 1, 5]\n",
    "max_depths = [1, 4, 8]\n",
    "\n",
    "args_list = [[path, *x] for x in list(itertools.product(learning_rates, max_depths))]\n",
    "print(args_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initiate `faculty_distributed.FacultyJobExecutor` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`faculty_distributed.FacultyJobExecutor` requires project id and job id to run. Optional arguments are clean, a boolean [default = True] that determines whether temporary files created for the run are deleted immediately after the completion of the job, and tmpdir_prefix, a string [default = '/project/.faculty-distributed'] that defines the path to where the temporary directory is created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fje = faculty_distributed.FacultyJobExecutor(project_id, job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute function on distributed workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call `map`, passing the function and the list of arguments, to execute the function. Once map has been called, a job will start with as many subruns as there are arguments passed. The output of these subruns will be returned as a list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = fje.map(train_and_predict, args_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyse results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are returned in the order of the aguments list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[accuracy_score(y_test, prediction) for prediction in predictions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove temporary data path created for this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
